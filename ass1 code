import numpy as np


def soft_max(x):
    exp_x = np.exp(x - np.max(x))
    softmax_values = exp_x / exp_x.sum(axis=0, keepdims=True)
    return softmax_values


n_labels = [0, 1, 2]


def Loss_sample_v2(W, b, x, y):
    vec = [np.dot(W[:, i], x) + b[i] for i in n_labels]
    denominator = sum([np.exp(a - max(vec))
                       for a in vec])
    return vec[y] - max(vec) - np.log(denominator)


def Loss_sample(W, b, x, y):
    vec = [np.dot(W[:, i], x) + b[i] for i in n_labels]
    denominator = sum([np.exp(a - max(vec))
                       for a in vec])
    return np.log(np.exp(vec[y] - max(vec)) / denominator)


def Loss_sm_reg(W, b, mb_samples, mb_lables):
    return 0 - sum(Loss_sample_v2(W, b, x, y) for (x, y) in zip(mb_samples, mb_lables))


list1 = np.array([1, 1, 1])
list2 = np.eye(3)
mat = np.eye(3)
b = np.zeros((3, 1))

W_example1 = np.array([[0.1, 0.2],
                      [0.3, 0.4],
                      [0.5, 0.6]])
W_example1 = W_example1.transpose()
b_example1 = np.array([0.01, 0.02, 0.03])
mb_samples_example1 = np.array([[1.0, 2.0],
                                [2.0, 1.0],
                                [3.0, 2.5]])
mb_labels_example1 = np.array([0, 1, 2])
print(Loss_sm_reg(W_example1, b_example1, mb_samples_example1, mb_labels_example1)/3)

print(f"expected output: {1.0704}")
